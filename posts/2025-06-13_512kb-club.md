# 512KB Club

#web
#performance
#puppeteer


> The 512KB Club is a collection of performance-focused web pages from across the Internet

The 512KB Club site has a leaderboard of sites from smallest to largest, all below 512KB in size. I came to wonder how
the size was determined, and therefore updated. Reading through how to add a site showed the process was to manually run
the site through [CloudFlare Radar](https://radar.cloudflare.com/scan)

I knew this meant the sites weren't getting updated... I checked the last-updated timestamps for some sites, many hadn't
been updated in years.

I thought this could be automated!



## Puppeteer

[Puppeteer](https://pptr.dev/) is a JS framework which runs a headless browser (Chrome or Firefox) and provides a JS
API to interface with it. It's event oriented, you add listeners for events like `{js}Network.loadingFinished`.

This seemed like it should be really easy. It quickly realised this wasn't the case, knowing which events to listen for
and which events gave access to which information required an unhealthy amount of trial and error. Claude was some help
but not a lot. It seems that the Chrome Developer Tools (CDT) events are just inconsistent. For example, I needed to
listen for both `{js}Network.responseReceived` and `{js}Network.loadingFinished`, the former gave details on the status code and
the later the request body size.

Then there were the edge cases, so many edge cases:

- `OPTIONS` requests don't have a body
- 404 requests only have their headers downloaded in Firefox but not Chrome
- Responses with a 204 (No Content) status code.
- Requests blocked by CSP
- Service workers
- Requests which pointed to dead domains
- Lazy-loaded elements required scrolling the page

After a while, I was able to get dev tools in the browser and my script to agree on total page weight. You can see the
full script [here](https://github.com/moebrowne/512-check/blob/master/puppeteer.js).



## The Results

I set the script running for all the URLs currently in The 512KB Club. This took a while, it could definitely benefit
from parallelism. The results were in:

```
- Total sites: 960
- Larger: 596 (>512: 82)
- Smaller: 241
- Unchanged: 0
- Dead: 35
- Error: 88
```

I was pretty unsurprised that most of the sites had got bigger. The full list of changed sites can be seen in the PR:

[<img style="max-width: 500px; margin: 0 auto;" src="https://opengraph.githubassets.com/545e2284f12fbbd454af9f6e8decdc857ab917ed425c5b7b6ce410c34fab60aa/kevquirk/512kb.club/pull/1830">](https://github.com/kevquirk/512kb.club/pull/1830)


## The Future

If I get around to it, I'd like to run all the sites through Firefox and Chrome and compare. There is also the issue of
the 88 sites which had an error, this isn't an error with the site but where something went wrong with my puppeteer
script aka more edge cases.
